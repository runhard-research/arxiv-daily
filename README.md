# arXiv Daily Papers

_Last updated: 2026-01-04_

---

## Computer Vision
<!-- START:vision -->
### DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments
- **arXiv**: http://arxiv.org/abs/2512.24985v1
- **Summary**:
  - Vision Language Models (VLMs) are increasingly adopted as central reasoning modules for embodied agents.
  - Existing benchmarks evaluate their capabilities under ideal, well-lit conditions, yet robust 24/7 operation demands performance under a wide range of visual degradations, including low-light conditions at night or in dark environments--a core necessity that has been largely overlooked.

<!-- END:vision -->

## Large Language Models
<!-- START:llm -->
### Encyclo-K: Evaluating LLMs with Dynamically Composed Knowledge Statements
- **arXiv**: http://arxiv.org/abs/2512.24867v1
- **Summary**:
  - Benchmarks play a crucial role in tracking the rapid advancement of large language models (LLMs) and identifying their capability boundaries.
  - However, existing benchmarks predominantly curate questions at the question level, suffering from three fundamental limitations: vulnerability to data contamination, restriction to single-knowledge-point assessment, and reliance on costly domain expert annotation.

### Compute-Accuracy Pareto Frontiers for Open-Source Reasoning Large Language Models
- **arXiv**: http://arxiv.org/abs/2512.24776v1
- **Summary**:
  - Large Language Models (LLMs) are demonstrating rapid improvements on complex reasoning benchmarks, particularly when allowed to utilize intermediate reasoning steps before converging on a final solution.
  - However, current literature often overlooks the significant computational burden associated with generating long reasoning sequences.

### Do Large Language Models Know What They Are Capable Of?
- **arXiv**: http://arxiv.org/abs/2512.24661v1
- **Summary**:
  - We investigate whether large language models (LLMs) can predict whether they will succeed on a given task and whether their predictions improve as they progress through multi-step tasks.
  - We also investigate whether LLMs can learn from in-context experiences to make better decisions about whether to pursue a task in scenarios where failure is costly.

### Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models
- **arXiv**: http://arxiv.org/abs/2512.24618v1
- **Summary**:
  - We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence.
  - Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities.

### Understanding and Steering the Cognitive Behaviors of Reasoning Models at Test-Time
- **arXiv**: http://arxiv.org/abs/2512.24574v1
- **Summary**:
  - Large Language Models (LLMs) often rely on long chain-of-thought (CoT) reasoning to solve complex tasks.
  - While effective, these trajectories are frequently inefficient, leading to high latency from excessive token generation, or unstable reasoning that alternates between underthinking (shallow, inconsistent steps) and overthinking (repetitive, verbose reasoning).

<!-- END:llm -->

## Multimodal (MLLM)
<!-- START:mllm -->
### DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments
- **arXiv**: http://arxiv.org/abs/2512.24985v1
- **Summary**:
  - Vision Language Models (VLMs) are increasingly adopted as central reasoning modules for embodied agents.
  - Existing benchmarks evaluate their capabilities under ideal, well-lit conditions, yet robust 24/7 operation demands performance under a wide range of visual degradations, including low-light conditions at night or in dark environments--a core necessity that has been largely overlooked.

### CPJ: Explainable Agricultural Pest Diagnosis via Caption-Prompt-Judge with LLM-Judged Refinement
- **arXiv**: http://arxiv.org/abs/2512.24947v1
- **Summary**:
  - Accurate and interpretable crop disease diagnosis is essential for agricultural decision-making, yet existing methods often rely on costly supervised fine-tuning and perform poorly under domain shifts.
  - We propose Caption--Prompt--Judge (CPJ), a training-free few-shot framework that enhances Agri-Pest VQA through structured, interpretable image captions.

### VLN-MME: Diagnosing MLLMs as Language-guided Visual Navigation agents
- **arXiv**: http://arxiv.org/abs/2512.24851v1
- **Summary**:
  - Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities across a wide range of vision-language tasks.
  - However, their performance as embodied agents, which requires multi-round dialogue spatial reasoning and sequential action prediction, needs further exploration.

### Video and Language Alignment in 2D Systems for 3D Multi-object Scenes with Multi-Information Derivative-Free Control
- **arXiv**: http://arxiv.org/abs/2512.24826v1
- **Summary**:
  - Cross-modal systems trained on 2D visual inputs are presented with a dimensional shift when processing 3D scenes.
  - An in-scene camera bridges the dimensionality gap but requires learning a control module.

<!-- END:mllm -->

## Vision + Robotics
<!-- START:vision_ro -->
### DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments
- **arXiv**: http://arxiv.org/abs/2512.24985v1
- **Summary**:
  - Vision Language Models (VLMs) are increasingly adopted as central reasoning modules for embodied agents.
  - Existing benchmarks evaluate their capabilities under ideal, well-lit conditions, yet robust 24/7 operation demands performance under a wide range of visual degradations, including low-light conditions at night or in dark environments--a core necessity that has been largely overlooked.

### VLN-MME: Diagnosing MLLMs as Language-guided Visual Navigation agents
- **arXiv**: http://arxiv.org/abs/2512.24851v1
- **Summary**:
  - Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities across a wide range of vision-language tasks.
  - However, their performance as embodied agents, which requires multi-round dialogue spatial reasoning and sequential action prediction, needs further exploration.

### Video and Language Alignment in 2D Systems for 3D Multi-object Scenes with Multi-Information Derivative-Free Control
- **arXiv**: http://arxiv.org/abs/2512.24826v1
- **Summary**:
  - Cross-modal systems trained on 2D visual inputs are presented with a dimensional shift when processing 3D scenes.
  - An in-scene camera bridges the dimensionality gap but requires learning a control module.

<!-- END:vision_ro -->
