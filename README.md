# arXiv Daily Papers

_Last updated: 2026-01-06_

---

## Computer Vision
<!-- START:vision -->
_No papers found._
<!-- END:vision -->

## Large Language Models
<!-- START:llm -->
### Agentic Memory: Learning Unified Long-Term and Short-Term Memory Management for Large Language Model Agents
- **arXiv**: http://arxiv.org/abs/2601.01885v1
- **Summary**:
  - Large language model (LLM) agents face fundamental limitations in long-horizon reasoning due to finite context windows, making effective memory management critical.
  - Existing methods typically handle long-term memory (LTM) and short-term memory (STM) as separate components, relying on heuristics or auxiliary controllers, which limits adaptability and end-to-end optimization.

<!-- END:llm -->

## Multimodal (MLLM)
<!-- START:mllm -->
### VINO: A Unified Visual Generator with Interleaved OmniModal Context
- **arXiv**: http://arxiv.org/abs/2601.02358v1
- **Summary**:
  - We present VINO, a unified visual generator that performs image and video generation and editing within a single framework.
  - Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model.

### Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes
- **arXiv**: http://arxiv.org/abs/2601.02356v1
- **Summary**:
  - We introduce Talk2Move, a reinforcement learning (RL) based diffusion framework for text-instructed spatial transformation of objects within scenes.
  - Spatially manipulating objects in a scene through natural language poses a challenge for multimodal generation systems.

### SLGNet: Synergizing Structural Priors and Language-Guided Modulation for Multimodal Object Detection
- **arXiv**: http://arxiv.org/abs/2601.02249v1
- **Summary**:
  - Multimodal object detection leveraging RGB and Infrared (IR) images is pivotal for robust perception in all-weather scenarios.
  - While recent adapter-based approaches efficiently transfer RGB-pretrained foundation models to this task, they often prioritize model efficiency at the expense of cross-modal structural consistency.

### BiPrompt: Bilateral Prompt Optimization for Visual and Textual Debiasing in Vision-Language Models
- **arXiv**: http://arxiv.org/abs/2601.02147v1
- **Summary**:
  - Vision language foundation models such as CLIP exhibit impressive zero-shot generalization yet remain vulnerable to spurious correlations across visual and textual modalities.
  - Existing debiasing approaches often address a single modality either visual or textual leading to partial robustness and unstable adaptation under distribution shifts.

<!-- END:mllm -->

## Vision + Robotics
<!-- START:vision_ro -->
### ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors
- **arXiv**: http://arxiv.org/abs/2601.02359v1
- **Summary**:
  - Detecting unknown deepfake manipulations remains one of the most challenging problems in face forgery detection.
  - Current state-of-the-art approaches fail to generalize to unseen manipulations, as they primarily rely on supervised training with existing deepfakes or pseudo-fakes, which leads to overfitting to specific forgery patterns.

### VINO: A Unified Visual Generator with Interleaved OmniModal Context
- **arXiv**: http://arxiv.org/abs/2601.02358v1
- **Summary**:
  - We present VINO, a unified visual generator that performs image and video generation and editing within a single framework.
  - Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model.

### Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes
- **arXiv**: http://arxiv.org/abs/2601.02356v1
- **Summary**:
  - We introduce Talk2Move, a reinforcement learning (RL) based diffusion framework for text-instructed spatial transformation of objects within scenes.
  - Spatially manipulating objects in a scene through natural language poses a challenge for multimodal generation systems.

### CycleVLA: Proactive Self-Correcting Vision-Language-Action Models via Subtask Backtracking and Minimum Bayes Risk Decoding
- **arXiv**: http://arxiv.org/abs/2601.02295v1
- **Summary**:
  - Current work on robot failure detection and correction typically operate in a post hoc manner, analyzing errors and applying corrections only after failures occur.
  - This work introduces CycleVLA, a system that equips Vision-Language-Action models (VLAs) with proactive self-correction, the capability to anticipate incipient failures and recover before they fully manifest during execution.

### InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams
- **arXiv**: http://arxiv.org/abs/2601.02281v1
- **Summary**:
  - The grand vision of enabling persistent, large-scale 3D visual geometry understanding is shackled by the irreconcilable demands of scalability and long-term stability.
  - While offline models like VGGT achieve inspiring geometry capability, their batch-based nature renders them irrelevant for live systems.

### DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies
- **arXiv**: http://arxiv.org/abs/2601.02267v1
- **Summary**:
  - Human mesh recovery from multi-view images faces a fundamental challenge: real-world datasets contain imperfect ground-truth annotations that bias the models' training, while synthetic data with precise supervision suffers from domain gap.
  - In this paper, we propose DiffProxy, a novel framework that generates multi-view consistent human proxies for mesh recovery.

### SLGNet: Synergizing Structural Priors and Language-Guided Modulation for Multimodal Object Detection
- **arXiv**: http://arxiv.org/abs/2601.02249v1
- **Summary**:
  - Multimodal object detection leveraging RGB and Infrared (IR) images is pivotal for robust perception in all-weather scenarios.
  - While recent adapter-based approaches efficiently transfer RGB-pretrained foundation models to this task, they often prioritize model efficiency at the expense of cross-modal structural consistency.

### NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation
- **arXiv**: http://arxiv.org/abs/2601.02204v1
- **Summary**:
  - We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens.
  - By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation.

### BiPrompt: Bilateral Prompt Optimization for Visual and Textual Debiasing in Vision-Language Models
- **arXiv**: http://arxiv.org/abs/2601.02147v1
- **Summary**:
  - Vision language foundation models such as CLIP exhibit impressive zero-shot generalization yet remain vulnerable to spurious correlations across visual and textual modalities.
  - Existing debiasing approaches often address a single modality either visual or textual leading to partial robustness and unstable adaptation under distribution shifts.

<!-- END:vision_ro -->
